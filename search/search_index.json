{
    "docs": [
        {
            "location": "/",
            "text": "About\n\n\nATLAS Analytics consists of numerous databases and services running at several distributed computing clusters, managed by different groups.\n\n\nThis documentation will try to provide one stop documentation to all our users.\nA brief overview of the system can be found  ...\n\n\nPlease feel free to update directly in the GitHub repository. To get access or rebuild the site please contact Ilija Vukotic.\n\n\nQuick links\n\n\n\n\n\n\n\n\nMonitoring\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\nbigPanda\n\n\nJupyter UC\n\n\n\n\n\n\nES UChicago\n\n\n\n\n\n\n\n\nES CERN\n\n\n\n\n\n\n\n\nMonit",
            "title": "Home"
        },
        {
            "location": "/#about",
            "text": "ATLAS Analytics consists of numerous databases and services running at several distributed computing clusters, managed by different groups.  This documentation will try to provide one stop documentation to all our users.\nA brief overview of the system can be found  ...  Please feel free to update directly in the GitHub repository. To get access or rebuild the site please contact Ilija Vukotic.",
            "title": "About"
        },
        {
            "location": "/#quick-links",
            "text": "Monitoring  Analytics      bigPanda  Jupyter UC    ES UChicago     ES CERN     Monit",
            "title": "Quick links"
        },
        {
            "location": "/sites/",
            "text": "Site administration\n\n\nJobs\n\n\nThere are around 100 different metrics we collect on all jobs. There are thousands of ways to filter and visualize them but for site admins we prepared one \ndashboard\n with the most important information:\n\n\n\n\nnumber of jobs in time, split by status. Look for total number of jobs, a lot of failed, canceled jobs\n\n\nfraction of different kinds of jobs (production, analysis, ...). This is important to know as they exhibit different stresses on the site, have different efficiencies, IO requirements.\n\n\nwall time delivered, cpu efficiency\n\n\nnumber of errors per node. Nodes with high percentage of failed jobs may need to be taken offline and cause investigated. \n\n\n\n\nKeep in mind that this dashboard contains only information on finished jobs. For not yet finished jobs one need to look \nhere\n.\nTo make it show only information from one sites add a line like this to the search bar: \nsitnamee:MWT2\n.\nWe suggest to try grouping nodes (based on CPU, storage, or network connectivity) and compare their performance.\n\n\nJobs IO\n\n\nSite movers collect information on all the input/output data transfers from the job. This data gets collected and indexed in ES at both CERN and UChicago. The data contains information on all the files that job accessesed or wrote, file sizes, rates, and a lot of metadata (filenames,workernode, etc.). Sites can use this data to:\n\n\n\n\nspot problematics WN (full or failed scratch disk, connection issue, missconfiguration)\n\n\nspot LAN issues (bad links, congested switches, ...)\n\n\noptimize access type (copy-to-scratch vs. direct access)\n\n\nmake purchasing decissions (switches, NICs, disks) \n\n\n\n\nStarting point should be this \nKibana dashboard\n. To make it show only information from one sites add a line like this to the search bar:\n   \nsitnamee:MWT2\n. First one would look at the visualization named \nerrors\n. Nodes with more than a few errors most probably need a deep inspection. \n\n\nFor a deeper investigation we suggest looking separately at WNs connected to the same switch. This is easiest done by making a search like this: \nsitename:MWT2 and hostname:uct2*\n. Big difference in performance can point to a problem with a switch WNs are connected to, or link from that switch to the storage. Feel free to make a special visualization that splits all your WNs in groups, save it (please prepend name of your site to names of your visualizations) so you don't have to redo it every day. You can also save a copy of the dashboard and customize it to your liking. \n\n\nWide Area Network issues\n\n\nIt is very important to quickly spot WAN connectivity issues. There are two easy ways to do it: \n\n\n\n\nSubscribe to an alert on network issues for your site. This check is described \nHERE\n and will alert you to low level problems (broken or saturated link, high packet loss, unstable path)\n\n\nLook at the FTS queues for your site. Long queues can point to bad settings in your FTS server, unusual Rucio requests, etc. Starting point for this kind of investigation is the \nFTS queues\n kibana page. Simply search for all transfers where your site is a source and then where your site is a destination (eg. \nsrc_site:MWT2\n or \ndest_site:MWT2\n). While a few files waiting in a queue for a few hours is not of big concern, a lot of transfers waiting for a long time (agreagate waiting time in years) is of concern.",
            "title": "Sites"
        },
        {
            "location": "/sites/#site-administration",
            "text": "",
            "title": "Site administration"
        },
        {
            "location": "/sites/#jobs",
            "text": "There are around 100 different metrics we collect on all jobs. There are thousands of ways to filter and visualize them but for site admins we prepared one  dashboard  with the most important information:   number of jobs in time, split by status. Look for total number of jobs, a lot of failed, canceled jobs  fraction of different kinds of jobs (production, analysis, ...). This is important to know as they exhibit different stresses on the site, have different efficiencies, IO requirements.  wall time delivered, cpu efficiency  number of errors per node. Nodes with high percentage of failed jobs may need to be taken offline and cause investigated.    Keep in mind that this dashboard contains only information on finished jobs. For not yet finished jobs one need to look  here .\nTo make it show only information from one sites add a line like this to the search bar:  sitnamee:MWT2 .\nWe suggest to try grouping nodes (based on CPU, storage, or network connectivity) and compare their performance.",
            "title": "Jobs"
        },
        {
            "location": "/sites/#jobs-io",
            "text": "Site movers collect information on all the input/output data transfers from the job. This data gets collected and indexed in ES at both CERN and UChicago. The data contains information on all the files that job accessesed or wrote, file sizes, rates, and a lot of metadata (filenames,workernode, etc.). Sites can use this data to:   spot problematics WN (full or failed scratch disk, connection issue, missconfiguration)  spot LAN issues (bad links, congested switches, ...)  optimize access type (copy-to-scratch vs. direct access)  make purchasing decissions (switches, NICs, disks)    Starting point should be this  Kibana dashboard . To make it show only information from one sites add a line like this to the search bar:\n    sitnamee:MWT2 . First one would look at the visualization named  errors . Nodes with more than a few errors most probably need a deep inspection.   For a deeper investigation we suggest looking separately at WNs connected to the same switch. This is easiest done by making a search like this:  sitename:MWT2 and hostname:uct2* . Big difference in performance can point to a problem with a switch WNs are connected to, or link from that switch to the storage. Feel free to make a special visualization that splits all your WNs in groups, save it (please prepend name of your site to names of your visualizations) so you don't have to redo it every day. You can also save a copy of the dashboard and customize it to your liking.",
            "title": "Jobs IO"
        },
        {
            "location": "/sites/#wide-area-network-issues",
            "text": "It is very important to quickly spot WAN connectivity issues. There are two easy ways to do it:    Subscribe to an alert on network issues for your site. This check is described  HERE  and will alert you to low level problems (broken or saturated link, high packet loss, unstable path)  Look at the FTS queues for your site. Long queues can point to bad settings in your FTS server, unusual Rucio requests, etc. Starting point for this kind of investigation is the  FTS queues  kibana page. Simply search for all transfers where your site is a source and then where your site is a destination (eg.  src_site:MWT2  or  dest_site:MWT2 ). While a few files waiting in a queue for a few hours is not of big concern, a lot of transfers waiting for a long time (agreagate waiting time in years) is of concern.",
            "title": "Wide Area Network issues"
        },
        {
            "location": "/ADC/",
            "text": "ADC\n\n\nFTS performance\n\n\nJob efficiency",
            "title": "ADC"
        },
        {
            "location": "/ADC/#adc",
            "text": "",
            "title": "ADC"
        },
        {
            "location": "/ADC/#fts-performance",
            "text": "",
            "title": "FTS performance"
        },
        {
            "location": "/ADC/#job-efficiency",
            "text": "",
            "title": "Job efficiency"
        },
        {
            "location": "/accounting/",
            "text": "Accounting\n\n\nJobs\n\n\nCPU\n\n\nNetwork\n\n\nStorage\n\n\nRucio make daily dumps of their main DB table containing all the datasets at all sites.  This data is in HDFS and is indexed at ES at UChicago. The raw data is kept for x days, while aggregated data is kept indefinitely. It must be kept in mind that the raw data repeats from day to day so while making time averages is OK, making sums over more than one day have no sense. \n Most important metrices are shown in this \ndashboard\n. \n  * Total storage per site in time\n  * Percentage of storage in space tokens per site. Percentage of storage in space tokens in time.\n  * Space occupied by different data formats, different tags.",
            "title": "Accounting"
        },
        {
            "location": "/accounting/#accounting",
            "text": "",
            "title": "Accounting"
        },
        {
            "location": "/accounting/#jobs",
            "text": "",
            "title": "Jobs"
        },
        {
            "location": "/accounting/#cpu",
            "text": "",
            "title": "CPU"
        },
        {
            "location": "/accounting/#network",
            "text": "",
            "title": "Network"
        },
        {
            "location": "/accounting/#storage",
            "text": "Rucio make daily dumps of their main DB table containing all the datasets at all sites.  This data is in HDFS and is indexed at ES at UChicago. The raw data is kept for x days, while aggregated data is kept indefinitely. It must be kept in mind that the raw data repeats from day to day so while making time averages is OK, making sums over more than one day have no sense. \n Most important metrices are shown in this  dashboard . \n  * Total storage per site in time\n  * Percentage of storage in space tokens per site. Percentage of storage in space tokens in time.\n  * Space occupied by different data formats, different tags.",
            "title": "Storage"
        },
        {
            "location": "/data-sources/Distributed_processing/",
            "text": "Distributed processing\n\n\nAll the code collecting and analyzing these datasets is in github repository: \nATLAS-Analytics/DistributedProcessing\n. \n\n\nJobs\n\n\nJobs data are imported from OracleDB table \nATLAS_PANDA.JOBSARCHIVED4\n once per hour. First data is sqooped to hdfs, then preprocessed and sent to UC Elasticsearch index.  \n\n\nEnrichments\n\n\nParent-child relationship\n\n\nWhen a job needs to be restarted for whatever reason, it gets a new pandaID. It can actually be restarted at more than one place. Information on parent/child relationship between jobs is stored in OracleDB table \nATLAS_PANDA.JEDI_JOB_RETRY_HISTORY\n. A sqoop job gets this info to HDFS from where a pyton codes reads it and uses it to update values in ES at UC.\n\nAt the moment code doing indexing is \nhere\n.\n\n\nJob status\n\n\nEvery time job changes a state (defined \nhere\n ) one row is added to \nATLAS_PANDA.JOBS_STATUSLOG\n table with the new state and timestamp.\nThis information is collected by a sqoop script, and saved in hdfs. Another pig script will read it from there, calculate time job spent in any of the states and add these times to jobs archive UC Elasticsearch indices. Variables are:\n\n\nTasks\n\n\nTasks data are imported once per hour from \nATLAS_PANDA.JEDI_TASKS\n Oracle table. \n\n\nEnrichments\n\n\nOutput formats\n\n\nIn order to understand what are perfromance characheristics of different derivation productions we need to addi output formats to the tasks. K8s cron job runs sqoop to get data from \nATLAS_DEFT.T_PRODUCTION_TASK\n and updates task table.",
            "title": "Distributed Processing"
        },
        {
            "location": "/data-sources/Distributed_processing/#distributed-processing",
            "text": "All the code collecting and analyzing these datasets is in github repository:  ATLAS-Analytics/DistributedProcessing .",
            "title": "Distributed processing"
        },
        {
            "location": "/data-sources/Distributed_processing/#jobs",
            "text": "Jobs data are imported from OracleDB table  ATLAS_PANDA.JOBSARCHIVED4  once per hour. First data is sqooped to hdfs, then preprocessed and sent to UC Elasticsearch index.",
            "title": "Jobs"
        },
        {
            "location": "/data-sources/Distributed_processing/#enrichments",
            "text": "",
            "title": "Enrichments"
        },
        {
            "location": "/data-sources/Distributed_processing/#parent-child-relationship",
            "text": "When a job needs to be restarted for whatever reason, it gets a new pandaID. It can actually be restarted at more than one place. Information on parent/child relationship between jobs is stored in OracleDB table  ATLAS_PANDA.JEDI_JOB_RETRY_HISTORY . A sqoop job gets this info to HDFS from where a pyton codes reads it and uses it to update values in ES at UC. At the moment code doing indexing is  here .",
            "title": "Parent-child relationship"
        },
        {
            "location": "/data-sources/Distributed_processing/#job-status",
            "text": "Every time job changes a state (defined  here  ) one row is added to  ATLAS_PANDA.JOBS_STATUSLOG  table with the new state and timestamp.\nThis information is collected by a sqoop script, and saved in hdfs. Another pig script will read it from there, calculate time job spent in any of the states and add these times to jobs archive UC Elasticsearch indices. Variables are:",
            "title": "Job status"
        },
        {
            "location": "/data-sources/Distributed_processing/#tasks",
            "text": "Tasks data are imported once per hour from  ATLAS_PANDA.JEDI_TASKS  Oracle table.",
            "title": "Tasks"
        },
        {
            "location": "/data-sources/Distributed_processing/#enrichments_1",
            "text": "",
            "title": "Enrichments"
        },
        {
            "location": "/data-sources/Distributed_processing/#output-formats",
            "text": "In order to understand what are perfromance characheristics of different derivation productions we need to addi output formats to the tasks. K8s cron job runs sqoop to get data from  ATLAS_DEFT.T_PRODUCTION_TASK  and updates task table.",
            "title": "Output formats"
        },
        {
            "location": "/data-sources/fts/",
            "text": "FTS\n\n\nDOCS\n\n\n\n\nMain google doc\n\n\nFTS3 configuration\n\n\nFTS3 Optimizer\n\n\n\n\nFTS servers\n\n\nConfigurations per FTS Server\n\n\n\n\nCERN\n\n\nBNL\n\n\n\n\nMonitoring @CERN\n\n\nMonit:\n\n\n\n\nGrafana\n\n\nKibana\n\n\n\n\nMonitoring @UC\n\n\nIndex pattern is \nfts-2*\n.\n\n\nTo check queue lenghts and average time in queue, check this dashboard:\nhttp://atlas-kibana-dev.mwt2.org/goto/5132d543e93e074899b0b92aa3cf1265\n\n\nTo check number of active transfers per activity type and per link, use index pattern \nlinks_traffic*\n.\n\n\nTo check ingress, egress, numbers of incomming and outcoming transfers per site, use index pattern \nsites_traffic*\n\nand this dashboard: http://atlas-kibana-dev.mwt2.org/goto/608b3f7a29b7b37afdd50a2c62fdd12c\n\n\nOLD DASHBOARDS (until March 2018)\n\n\nGlobally most important \nFTS queue waits.\n\n\nComparissons of average queue times for transfers where destination is \nUS or not US.\n\n\nSite specific dashboards:\n\n\n\n\nMWT2 \nQueues\n \nTransfers\n\n\nAGLT2 \nQueues\n \nTransfers\n\n\nBNL \nTransfers\n\n\n\n\nMonitoring flow description\n\n\nThe new data flow started March 2018. Data is collected from the FTS AMQ and sent directly to ES at UC, so it is almost real-time. All the IDs are stored so results can be crosschecked against both RUCIO and FTS.",
            "title": "FTS"
        },
        {
            "location": "/data-sources/fts/#fts",
            "text": "",
            "title": "FTS"
        },
        {
            "location": "/data-sources/fts/#docs",
            "text": "Main google doc  FTS3 configuration  FTS3 Optimizer",
            "title": "DOCS"
        },
        {
            "location": "/data-sources/fts/#fts-servers",
            "text": "Configurations per FTS Server   CERN  BNL",
            "title": "FTS servers"
        },
        {
            "location": "/data-sources/fts/#monitoring-cern",
            "text": "Monit:   Grafana  Kibana",
            "title": "Monitoring @CERN"
        },
        {
            "location": "/data-sources/fts/#monitoring-uc",
            "text": "Index pattern is  fts-2* .  To check queue lenghts and average time in queue, check this dashboard:\nhttp://atlas-kibana-dev.mwt2.org/goto/5132d543e93e074899b0b92aa3cf1265  To check number of active transfers per activity type and per link, use index pattern  links_traffic* .  To check ingress, egress, numbers of incomming and outcoming transfers per site, use index pattern  sites_traffic* \nand this dashboard: http://atlas-kibana-dev.mwt2.org/goto/608b3f7a29b7b37afdd50a2c62fdd12c",
            "title": "Monitoring @UC"
        },
        {
            "location": "/data-sources/fts/#old-dashboards-until-march-2018",
            "text": "Globally most important  FTS queue waits.  Comparissons of average queue times for transfers where destination is  US or not US.  Site specific dashboards:   MWT2  Queues   Transfers  AGLT2  Queues   Transfers  BNL  Transfers",
            "title": "OLD DASHBOARDS (until March 2018)"
        },
        {
            "location": "/data-sources/fts/#monitoring-flow-description",
            "text": "The new data flow started March 2018. Data is collected from the FTS AMQ and sent directly to ES at UC, so it is almost real-time. All the IDs are stored so results can be crosschecked against both RUCIO and FTS.",
            "title": "Monitoring flow description"
        },
        {
            "location": "/data-sources/perfsonar/",
            "text": "PerfSONAR\n\n\nData collection\n\n\nPerfsonar data gets sent to Nebraska RabbitMQ. From there it gets sent to Fermilab for storage on tape, to ES at UChicago and to ES at Nebraska. \n\n\nCollectors are in a docker container auto-built in dockerHub:              \nivukotic/perfsonar_collectors\n\n\nData analysis\n\n\nThere is a whole docker container with a number of tools and services:         \nopensciencegrid/network_analytics\n \n\n\nIt has apache web server, neo4j server, jupyter lab server. All of it is easily deployable on a k8s cluster.",
            "title": "PerfSONAR"
        },
        {
            "location": "/data-sources/perfsonar/#perfsonar",
            "text": "",
            "title": "PerfSONAR"
        },
        {
            "location": "/data-sources/perfsonar/#data-collection",
            "text": "Perfsonar data gets sent to Nebraska RabbitMQ. From there it gets sent to Fermilab for storage on tape, to ES at UChicago and to ES at Nebraska.   Collectors are in a docker container auto-built in dockerHub:               ivukotic/perfsonar_collectors",
            "title": "Data collection"
        },
        {
            "location": "/data-sources/perfsonar/#data-analysis",
            "text": "There is a whole docker container with a number of tools and services:          opensciencegrid/network_analytics    It has apache web server, neo4j server, jupyter lab server. All of it is easily deployable on a k8s cluster.",
            "title": "Data analysis"
        },
        {
            "location": "/data-sources/xaod/",
            "text": "xAOD monitoring\n\n\nCode that is needed in order to access xAOD data has been instrumented (WHERE???) so it collects information on containers and branches accessed by the user.\n\n\nThis data is reported to a RUCIO appache server that stores it in HDFS. (Who takes care of this??? Where is it documented???)\n\n\nOnce a day information from HDFS is processed using a pig script and a python code and sent to Elasticsearch cluster at UChicago. This code can be found in \nGitHub\n.\n\n\nHDFS @CERN\n\n\nData is in: \nhdfs://analytix//user/rucio01/nongrid_traces/$INPD.json\n\n\nElasticsearch at UChicago\n\n\nData is stored in two indices:\n\n\n\n\n\n\ninformation per job.  Index is named: \nxaod_accesses\n  Template is \nhere\n. Data is produced and indexed directly by the pig script. \n\n\n\n\n\n\ncontainer or branch access. Index is named: \nxaod_job_accesses\n  Template is \nhere\n.\n\n\n\n\n\n\nDashboards\n\n\nAnalysis\n\n\nHere links to relevant Jupyter notebooks.1",
            "title": "xAOD"
        },
        {
            "location": "/data-sources/xaod/#xaod-monitoring",
            "text": "Code that is needed in order to access xAOD data has been instrumented (WHERE???) so it collects information on containers and branches accessed by the user.  This data is reported to a RUCIO appache server that stores it in HDFS. (Who takes care of this??? Where is it documented???)  Once a day information from HDFS is processed using a pig script and a python code and sent to Elasticsearch cluster at UChicago. This code can be found in  GitHub .",
            "title": "xAOD monitoring"
        },
        {
            "location": "/data-sources/xaod/#hdfs-cern",
            "text": "Data is in:  hdfs://analytix//user/rucio01/nongrid_traces/$INPD.json",
            "title": "HDFS @CERN"
        },
        {
            "location": "/data-sources/xaod/#elasticsearch-at-uchicago",
            "text": "Data is stored in two indices:    information per job.  Index is named:  xaod_accesses   Template is  here . Data is produced and indexed directly by the pig script.     container or branch access. Index is named:  xaod_job_accesses   Template is  here .",
            "title": "Elasticsearch at UChicago"
        },
        {
            "location": "/data-sources/xaod/#dashboards",
            "text": "",
            "title": "Dashboards"
        },
        {
            "location": "/data-sources/xaod/#analysis",
            "text": "Here links to relevant Jupyter notebooks.1",
            "title": "Analysis"
        },
        {
            "location": "/data-sources/benchmarks/",
            "text": "CPU benchmarks\n\n\nFor 1% of grid jobs pilot in parallel to fetching input data, runs three different benchmarks. These are:\n * fastBMK\n * \n * \nIt also collects data on node:\n\n\n\n\nCPU type\n\n\nis it VM\n\n\nis it hyperthreaded\n\n\n\n\n\n\nAll the info gets sent to a logstash instance running at uct2-collectd.mwt2.org and from there to ES.",
            "title": "Benchmarks"
        },
        {
            "location": "/data-sources/benchmarks/#cpu-benchmarks",
            "text": "For 1% of grid jobs pilot in parallel to fetching input data, runs three different benchmarks. These are:\n * fastBMK\n * \n * \nIt also collects data on node:   CPU type  is it VM  is it hyperthreaded    All the info gets sent to a logstash instance running at uct2-collectd.mwt2.org and from there to ES.",
            "title": "CPU benchmarks"
        },
        {
            "location": "/data-sources/rtt/",
            "text": "",
            "title": "RTT"
        },
        {
            "location": "/data-sources/esnet/",
            "text": "ESnet\n\n\nData collection\n\n\nThanks to Jon Dugan we can collect data from ESnet \nREST interface\n and send it to ES at UChicago. \nCollector is in \nGitHub repo\n while a docker container is auto-built in dockerHub: \natlasanalyticsservice/esnet\n.\n\n\nCurrently the collector runs at es-docker.mwt2.org but will be moved to a kubernetes cluster at MWT2 as soon as we have it.\n\n\nData analysis\n\n\nWe still did not start analyzing this data. Once we do, it will be done using tools and services of docker container:        \nopensciencegrid/network_analytics",
            "title": "ESnet"
        },
        {
            "location": "/data-sources/esnet/#esnet",
            "text": "",
            "title": "ESnet"
        },
        {
            "location": "/data-sources/esnet/#data-collection",
            "text": "Thanks to Jon Dugan we can collect data from ESnet  REST interface  and send it to ES at UChicago. \nCollector is in  GitHub repo  while a docker container is auto-built in dockerHub:  atlasanalyticsservice/esnet .  Currently the collector runs at es-docker.mwt2.org but will be moved to a kubernetes cluster at MWT2 as soon as we have it.",
            "title": "Data collection"
        },
        {
            "location": "/data-sources/esnet/#data-analysis",
            "text": "We still did not start analyzing this data. Once we do, it will be done using tools and services of docker container:         opensciencegrid/network_analytics",
            "title": "Data analysis"
        },
        {
            "location": "/data-sources/DDM/",
            "text": "DDM monitoring\n\n\nIs it rucio sending it directly to ES at UC?\nWhere is aggregation running?",
            "title": "DDM"
        },
        {
            "location": "/data-sources/DDM/#ddm-monitoring",
            "text": "Is it rucio sending it directly to ES at UC?\nWhere is aggregation running?",
            "title": "DDM monitoring"
        },
        {
            "location": "/infrastructure/computing_nodes/",
            "text": "Old infrastructure\n\n\nNodes\n\n\naianalytics nodes\n\n\nOpenStack environment variables set for Project = 'ATLAS Services Analytics'\n\n\n\n\n\n\n\n\nNode\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\naianalytics01\n\n\ncern-geneva-b\n\n\n\n\n\n\naianalytics03\n\n\ncern-geneva-b\n\n\n\n\n\n\naianalytics10\n\n\ncern-geneva-a\n\n\n\n\n\n\naianalytics12\n\n\ncern-wigner-a\n\n\n\n\n\n\naianalytics13\n\n\ncern-wigner-a\n\n\n\n\n\n\naianalytics14\n\n\ncern-geneva-c\n\n\n\n\n\n\naianalytics15\n\n\ncern-geneva-c\n\n\n\n\n\n\naianalytics16\n\n\ncern-geneva-a\n\n\n\n\n\n\n\n\naiatlas013 | cern-wigner-b \naiatlas025 | cern-wigner-b \n\n\n\n\naiatlas114.cern.ch\n\n\n\n\nAccounts\n\n\nadcmusr3 is a fax monitoring account running things on aiatlas114.cern.ch\n  aflume - service account \n\n\nNew infrastructure\n\n\nThe new infrastructure is based on OpenStack project \"ATLAS AnalyticsSvc\". It has five m2.medium nodes.\n\n\nKubernetes cluster\n\n\nAll the nodes are under \"analytics\" kubernetes cluster based on kubernetes-preview template. One node is a master and 4 nodes are minions. \nThe cluster is monitored using Heapster which sends data to ES at UC. Dashboard showing its data is \nhere\n. \n\n\nUsing your private kubernetes cluster for analytics\n\n\nWhile there is not much documentation the procedure is straightforward. Here the basics:\n\n\n\n\nGetting your OpenStack resources\nhttps://openstack.cern.ch/\n\n\nSeting up kubernetes cluster\n\n\nInstalling kubectl and basic commands\n\n\nUsing Analytics container \n\n\n\n\nAccounts\n\n\n\n\n\n\n\n\nName\n\n\nLogin\n\n\nEmail\n\n\n\n\n\n\n\n\n\n\nATLAS analytics service\n\n\nanalyticssvc\n\n\nanalytics.service@cern.ch\n\n\n\n\n\n\nATLAS analytics\n\n\nanalytics\n\n\natlas.analytics@cern.ch\n\n\n\n\n\n\n\n\nSERVICES\n\n\nTo be moved from the old infrastructure\n\n FTS - old one to be removed (acrontab)\n\n PerfSONAR - old ones running on aianalytics10.\n\n\nAlready running on new infrastructure:\n\n BOINC, BOINC-dev\n\n Job, Task, Enrichment collectors\n\n FTS\n\n DDM accounting\n* xAOD",
            "title": "Backend nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#old-infrastructure",
            "text": "",
            "title": "Old infrastructure"
        },
        {
            "location": "/infrastructure/computing_nodes/#nodes",
            "text": "",
            "title": "Nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#aianalytics-nodes",
            "text": "OpenStack environment variables set for Project = 'ATLAS Services Analytics'     Node  Location      aianalytics01  cern-geneva-b    aianalytics03  cern-geneva-b    aianalytics10  cern-geneva-a    aianalytics12  cern-wigner-a    aianalytics13  cern-wigner-a    aianalytics14  cern-geneva-c    aianalytics15  cern-geneva-c    aianalytics16  cern-geneva-a     aiatlas013 | cern-wigner-b \naiatlas025 | cern-wigner-b    aiatlas114.cern.ch",
            "title": "aianalytics nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#accounts",
            "text": "adcmusr3 is a fax monitoring account running things on aiatlas114.cern.ch\n  aflume - service account",
            "title": "Accounts"
        },
        {
            "location": "/infrastructure/computing_nodes/#new-infrastructure",
            "text": "The new infrastructure is based on OpenStack project \"ATLAS AnalyticsSvc\". It has five m2.medium nodes.",
            "title": "New infrastructure"
        },
        {
            "location": "/infrastructure/computing_nodes/#kubernetes-cluster",
            "text": "All the nodes are under \"analytics\" kubernetes cluster based on kubernetes-preview template. One node is a master and 4 nodes are minions. \nThe cluster is monitored using Heapster which sends data to ES at UC. Dashboard showing its data is  here .",
            "title": "Kubernetes cluster"
        },
        {
            "location": "/infrastructure/computing_nodes/#using-your-private-kubernetes-cluster-for-analytics",
            "text": "While there is not much documentation the procedure is straightforward. Here the basics:   Getting your OpenStack resources\nhttps://openstack.cern.ch/  Seting up kubernetes cluster  Installing kubectl and basic commands  Using Analytics container",
            "title": "Using your private kubernetes cluster for analytics"
        },
        {
            "location": "/infrastructure/computing_nodes/#accounts_1",
            "text": "Name  Login  Email      ATLAS analytics service  analyticssvc  analytics.service@cern.ch    ATLAS analytics  analytics  atlas.analytics@cern.ch",
            "title": "Accounts"
        },
        {
            "location": "/infrastructure/computing_nodes/#services",
            "text": "To be moved from the old infrastructure  FTS - old one to be removed (acrontab)  PerfSONAR - old ones running on aianalytics10.  Already running on new infrastructure:  BOINC, BOINC-dev  Job, Task, Enrichment collectors  FTS  DDM accounting\n* xAOD",
            "title": "SERVICES"
        },
        {
            "location": "/infrastructure/analytics/",
            "text": "",
            "title": "Analytics"
        },
        {
            "location": "/infrastructure/amq/",
            "text": "",
            "title": "AMQ"
        },
        {
            "location": "/infrastructure/elasticsearch/",
            "text": "Elasticsearch\n\n\nUChicago\n\n\nInfrastructure\n\n\n\n\n5 data nodes, 3 service nodes (3 masters, 2 kibana), 15 TB of SSDs. Will be expanded to:...\n\n\nbackup: each Sunday all data is incrementally backed up.\n\n\n\n\nKibana\n\n\n\n\nProduction\n\n\n\n\nProduction version\n is fully open but only for read access. Searches, visualization, dashboards can be created but can not be saved. To add something to it one has to create it first in the development and ask Ilija to copy it over to the production Kibana.\n\n\n\n\nDevelopment\n\nDevelopment version\n is read/write capable but sits behind a apache proxy, if you need a password ask Ilija Vukotic.\n\n\n\n\nIT-ES\n\n\nMonit",
            "title": "Elasticsearch"
        },
        {
            "location": "/infrastructure/elasticsearch/#elasticsearch",
            "text": "",
            "title": "Elasticsearch"
        },
        {
            "location": "/infrastructure/elasticsearch/#uchicago",
            "text": "",
            "title": "UChicago"
        },
        {
            "location": "/infrastructure/elasticsearch/#infrastructure",
            "text": "5 data nodes, 3 service nodes (3 masters, 2 kibana), 15 TB of SSDs. Will be expanded to:...  backup: each Sunday all data is incrementally backed up.",
            "title": "Infrastructure"
        },
        {
            "location": "/infrastructure/elasticsearch/#kibana",
            "text": "Production   Production version  is fully open but only for read access. Searches, visualization, dashboards can be created but can not be saved. To add something to it one has to create it first in the development and ask Ilija to copy it over to the production Kibana.   Development Development version  is read/write capable but sits behind a apache proxy, if you need a password ask Ilija Vukotic.",
            "title": "Kibana"
        },
        {
            "location": "/infrastructure/elasticsearch/#it-es",
            "text": "",
            "title": "IT-ES"
        },
        {
            "location": "/infrastructure/elasticsearch/#monit",
            "text": "",
            "title": "Monit"
        },
        {
            "location": "/infrastructure/hadoop/",
            "text": "",
            "title": "Hadoop"
        },
        {
            "location": "/infrastructure/logstash/",
            "text": "",
            "title": "Logstash"
        },
        {
            "location": "/alarms_and_alerts/",
            "text": "Alarms & Alerts\n\n\nWe are periodically running a number of different checks. When these checks fail we generate an \nAlarm\n. When a person gets an e-mail concerning an alarm, we call that an \nAlert\n. \nAt the moment tests are running as cron jobs on our main analytics node (uct3-lx2.uchicago.edu). \nSystem is described \nhere\n.\nTo subscribe to alerts use this \nlink\n. Each mail you recieve will contain a link that one can use to change mailing preferences.\n\n\nNetwork issues\n\n\nToo large packet loss to multiple sites\n\n\nData collection issues\n\n\nPerfSONAR\n\n\nJobs and Tasks\n\n\nFrontier & Squid issues\n\n\nUser caused issues\n\n\nElasticsearch issues",
            "title": "Alarms and Alerts"
        },
        {
            "location": "/alarms_and_alerts/#alarms-alerts",
            "text": "We are periodically running a number of different checks. When these checks fail we generate an  Alarm . When a person gets an e-mail concerning an alarm, we call that an  Alert . \nAt the moment tests are running as cron jobs on our main analytics node (uct3-lx2.uchicago.edu). \nSystem is described  here .\nTo subscribe to alerts use this  link . Each mail you recieve will contain a link that one can use to change mailing preferences.",
            "title": "Alarms &amp; Alerts"
        },
        {
            "location": "/alarms_and_alerts/#network-issues",
            "text": "",
            "title": "Network issues"
        },
        {
            "location": "/alarms_and_alerts/#too-large-packet-loss-to-multiple-sites",
            "text": "",
            "title": "Too large packet loss to multiple sites"
        },
        {
            "location": "/alarms_and_alerts/#data-collection-issues",
            "text": "",
            "title": "Data collection issues"
        },
        {
            "location": "/alarms_and_alerts/#perfsonar",
            "text": "",
            "title": "PerfSONAR"
        },
        {
            "location": "/alarms_and_alerts/#jobs-and-tasks",
            "text": "",
            "title": "Jobs and Tasks"
        },
        {
            "location": "/alarms_and_alerts/#frontier-squid-issues",
            "text": "",
            "title": "Frontier &amp; Squid issues"
        },
        {
            "location": "/alarms_and_alerts/#user-caused-issues",
            "text": "",
            "title": "User caused issues"
        },
        {
            "location": "/alarms_and_alerts/#elasticsearch-issues",
            "text": "",
            "title": "Elasticsearch issues"
        },
        {
            "location": "/user-guide/es_and_kibana/",
            "text": "ES and Kibana",
            "title": "Elasticsearch & Kibana"
        },
        {
            "location": "/user-guide/es_and_kibana/#es-and-kibana",
            "text": "",
            "title": "ES and Kibana"
        },
        {
            "location": "/user-guide/hadoop/",
            "text": "",
            "title": "Hadoop"
        },
        {
            "location": "/user-guide/spark/",
            "text": "",
            "title": "Spark"
        },
        {
            "location": "/user-guide/pig/",
            "text": "",
            "title": "Pig Latin"
        },
        {
            "location": "/user-guide/jupyter/",
            "text": "",
            "title": "Jupyter"
        }
    ]
}