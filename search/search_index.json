{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to ATLAS Analytics documentation\n\n\nATLAS Analytics consists of numerous databases and services running at several distributed computing clusters, managed by different groups.\n\n\nThis documentation will try to provide one stop documentation to all our users.\nA brief overview of the system can be found  ...\n\n\nPlease feel free to update directly in the GitHub repository. To get access or rebuild the site please contact Ilija Vukotic.\n\n\nQuick links\n\n\n\n\n\n\n\n\nMonitoring\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\nbigPanda\n\n\nJupyter UC\n\n\n\n\n\n\nUC ES",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-atlas-analytics-documentation",
            "text": "ATLAS Analytics consists of numerous databases and services running at several distributed computing clusters, managed by different groups.  This documentation will try to provide one stop documentation to all our users.\nA brief overview of the system can be found  ...  Please feel free to update directly in the GitHub repository. To get access or rebuild the site please contact Ilija Vukotic.",
            "title": "Welcome to ATLAS Analytics documentation"
        },
        {
            "location": "/#quick-links",
            "text": "Monitoring  Analytics      bigPanda  Jupyter UC    UC ES",
            "title": "Quick links"
        },
        {
            "location": "/data-sources/Distributed_processing/",
            "text": "Distributed processing\n\n\nJobs\n\n\nTasks",
            "title": "Distributed Processing"
        },
        {
            "location": "/data-sources/Distributed_processing/#distributed-processing",
            "text": "",
            "title": "Distributed processing"
        },
        {
            "location": "/data-sources/Distributed_processing/#jobs",
            "text": "",
            "title": "Jobs"
        },
        {
            "location": "/data-sources/Distributed_processing/#tasks",
            "text": "",
            "title": "Tasks"
        },
        {
            "location": "/data-sources/fts/",
            "text": "FTS\n\n\nDOCS\n\n\n\n\nMain google doc\n\n\nFTS3 configuration\n\n\nFTS3 Optimizer\n\n\n\n\nFTS servers\n\n\nConfigurations per FTS Server\n\n\n\n\nCERN\n\n\nBNL\n\n\n\n\nMonitoring @CERN\n\n\nMonit:\n\n\n\n\nGrafana\n\n\nKibana\n\n\n\n\nMonitoring @UC\n\n\nGlobally most important \nFTS queue waits.\n\n\nComparissons of average queue times for transfers where destination is \nUS or not US.\n\n\nSite specific dashboards:\n\n\n\n\nMWT2 \nQueues\n \nTransfers\n\n\nAGLT2 \nQueues\n \nTransfers\n\n\nBNL \nTransfers\n\n\n\n\nMonitoring flow description",
            "title": "FTS"
        },
        {
            "location": "/data-sources/fts/#fts",
            "text": "",
            "title": "FTS"
        },
        {
            "location": "/data-sources/fts/#docs",
            "text": "Main google doc  FTS3 configuration  FTS3 Optimizer",
            "title": "DOCS"
        },
        {
            "location": "/data-sources/fts/#fts-servers",
            "text": "Configurations per FTS Server   CERN  BNL",
            "title": "FTS servers"
        },
        {
            "location": "/data-sources/fts/#monitoring-cern",
            "text": "Monit:   Grafana  Kibana",
            "title": "Monitoring @CERN"
        },
        {
            "location": "/data-sources/fts/#monitoring-uc",
            "text": "Globally most important  FTS queue waits.  Comparissons of average queue times for transfers where destination is  US or not US.  Site specific dashboards:   MWT2  Queues   Transfers  AGLT2  Queues   Transfers  BNL  Transfers",
            "title": "Monitoring @UC"
        },
        {
            "location": "/data-sources/fts/#monitoring-flow-description",
            "text": "",
            "title": "Monitoring flow description"
        },
        {
            "location": "/data-sources/perfsonar/",
            "text": "PerfSONAR\n\n\nData collection\n\n\nPerfsonar data gets sent to Nebraska RabbitMQ. From there it gets sent to Fermilab for storage on tape, to ES at UChicago and to ES at Nebraska.\n\nCollectors are in a docker container auto-built in dockerHub: ivukotic/perfsonar_collectors\n\n\n\nData analysis\n\n\nThere is a whole docker container with a number of tools and services: opensciencegrid/network_analytics \nIt has apache web server, neo4j server, jupyter lab server. All of it is easily deployable on a k8s cluster.",
            "title": "PerfSONAR"
        },
        {
            "location": "/data-sources/perfsonar/#perfsonar",
            "text": "",
            "title": "PerfSONAR"
        },
        {
            "location": "/data-sources/perfsonar/#data-collection",
            "text": "Perfsonar data gets sent to Nebraska RabbitMQ. From there it gets sent to Fermilab for storage on tape, to ES at UChicago and to ES at Nebraska.\n\nCollectors are in a docker container auto-built in dockerHub: ivukotic/perfsonar_collectors",
            "title": "Data collection"
        },
        {
            "location": "/data-sources/perfsonar/#data-analysis",
            "text": "There is a whole docker container with a number of tools and services: opensciencegrid/network_analytics \nIt has apache web server, neo4j server, jupyter lab server. All of it is easily deployable on a k8s cluster.",
            "title": "Data analysis"
        },
        {
            "location": "/data-sources/benchmarks/",
            "text": "",
            "title": "Benchmarks"
        },
        {
            "location": "/data-sources/xaod/",
            "text": "xAOD monitoring\n\n\nCode that is needed in order to access xAOD data has been instrumented (WHERE???) so it collects information on containers and branches accessed by the user.\n\n\nThis data is reported to a RUCIO appache server that stores it in HDFS. (Who takes care of this??? Where is it documented???)\n\n\nOnce a day information from HDFS is processed using a pig script and a python code and sent to Elasticsearch cluster at UChicago. This code can be found in \nGitHub\n.\n\n\nHDFS @CERN\n\n\nData is in: \nhdfs://analytix//user/rucio01/nongrid_traces/$INPD.json\n\n\nElasticsearch at UChicago\n\n\nData is stored in two indices:\n\n\n\n\n\n\ninformation per job.  Index is named: \nxaod_accesses\n  Template is \nhere\n. Data is produced and indexed directly by the pig script. \n\n\n\n\n\n\ncontainer or branch access. Index is named: \nxaod_job_accesses\n  Template is \nhere\n.\n\n\n\n\n\n\nDashboards\n\n\nAnalysis\n\n\nHere links to relevant Jupyter notebooks.",
            "title": "xAOD"
        },
        {
            "location": "/data-sources/xaod/#xaod-monitoring",
            "text": "Code that is needed in order to access xAOD data has been instrumented (WHERE???) so it collects information on containers and branches accessed by the user.  This data is reported to a RUCIO appache server that stores it in HDFS. (Who takes care of this??? Where is it documented???)  Once a day information from HDFS is processed using a pig script and a python code and sent to Elasticsearch cluster at UChicago. This code can be found in  GitHub .",
            "title": "xAOD monitoring"
        },
        {
            "location": "/data-sources/xaod/#hdfs-cern",
            "text": "Data is in:  hdfs://analytix//user/rucio01/nongrid_traces/$INPD.json",
            "title": "HDFS @CERN"
        },
        {
            "location": "/data-sources/xaod/#elasticsearch-at-uchicago",
            "text": "Data is stored in two indices:    information per job.  Index is named:  xaod_accesses   Template is  here . Data is produced and indexed directly by the pig script.     container or branch access. Index is named:  xaod_job_accesses   Template is  here .",
            "title": "Elasticsearch at UChicago"
        },
        {
            "location": "/data-sources/xaod/#dashboards",
            "text": "",
            "title": "Dashboards"
        },
        {
            "location": "/data-sources/xaod/#analysis",
            "text": "Here links to relevant Jupyter notebooks.",
            "title": "Analysis"
        },
        {
            "location": "/data-sources/rtt/",
            "text": "",
            "title": "RTT"
        },
        {
            "location": "/infrastructure/computing_nodes/",
            "text": "Old infrastructure\n\n\nNodes\n\n\naianalytics nodes\n\n\nOpenStack environment variables set for Project = 'ATLAS Services Analytics'\n\n\n\n\n\n\n\n\nNode\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\naianalytics01\n\n\ncern-geneva-b\n\n\n\n\n\n\naianalytics03\n\n\ncern-geneva-b\n\n\n\n\n\n\naianalytics10\n\n\ncern-geneva-a\n\n\n\n\n\n\naianalytics12\n\n\ncern-wigner-a\n\n\n\n\n\n\naianalytics13\n\n\ncern-wigner-a\n\n\n\n\n\n\naianalytics14\n\n\ncern-geneva-c\n\n\n\n\n\n\naianalytics15\n\n\ncern-geneva-c\n\n\n\n\n\n\naianalytics16\n\n\ncern-geneva-a\n\n\n\n\n\n\n\n\naiatlas013 | cern-wigner-b \naiatlas025 | cern-wigner-b \n\n\n\n\naiatlas114.cern.ch\n\n\n\n\nAccounts\n\n\nadcmusr3 is a fax monitoring account running things on aiatlas114.cern.ch\n  aflume - service account \n\n\nNew infrastructure\n\n\nNodes\n\n\nAccounts\n\n\n\n\n\n\n\n\nName\n\n\nLogin\n\n\nEmail\n\n\n\n\n\n\n\n\n\n\nATLAS analytics service\n\n\nanalyticssvc\n\n\nanalytics.service@cern.ch\n\n\n\n\n\n\nATLAS analytics\n\n\nanalytics\n\n\natlas.analytics@cern.ch",
            "title": "Computing"
        },
        {
            "location": "/infrastructure/computing_nodes/#old-infrastructure",
            "text": "",
            "title": "Old infrastructure"
        },
        {
            "location": "/infrastructure/computing_nodes/#nodes",
            "text": "",
            "title": "Nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#aianalytics-nodes",
            "text": "OpenStack environment variables set for Project = 'ATLAS Services Analytics'     Node  Location      aianalytics01  cern-geneva-b    aianalytics03  cern-geneva-b    aianalytics10  cern-geneva-a    aianalytics12  cern-wigner-a    aianalytics13  cern-wigner-a    aianalytics14  cern-geneva-c    aianalytics15  cern-geneva-c    aianalytics16  cern-geneva-a     aiatlas013 | cern-wigner-b \naiatlas025 | cern-wigner-b    aiatlas114.cern.ch",
            "title": "aianalytics nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#accounts",
            "text": "adcmusr3 is a fax monitoring account running things on aiatlas114.cern.ch\n  aflume - service account",
            "title": "Accounts"
        },
        {
            "location": "/infrastructure/computing_nodes/#new-infrastructure",
            "text": "",
            "title": "New infrastructure"
        },
        {
            "location": "/infrastructure/computing_nodes/#nodes_1",
            "text": "",
            "title": "Nodes"
        },
        {
            "location": "/infrastructure/computing_nodes/#accounts_1",
            "text": "Name  Login  Email      ATLAS analytics service  analyticssvc  analytics.service@cern.ch    ATLAS analytics  analytics  atlas.analytics@cern.ch",
            "title": "Accounts"
        },
        {
            "location": "/infrastructure/amq/",
            "text": "",
            "title": "AMQ"
        },
        {
            "location": "/infrastructure/elasticsearch/",
            "text": "Elasticsearch\n\n\nUChicago\n\n\nInfrastructure\n\n\n\n\n5 data nodes, 3 service nodes (3 masters, 2 kibana), 15 TB of SSDs. Will be expanded to:...\n\n\nbackup: each Sunday all data is incrementally backed up.\n\n\n\n\nKibana\n\n\n\n\nProduction\n\n\n\n\nProduction version\n is fully open but only for read access. Searches, visualization, dashboards can be created but can not be saved. To add something to it one has to create it first in the development and ask Ilija to copy it over to the production Kibana.\n\n\n\n\nDevelopment\n\nDevelopment version\n is read/write capable but sits behind a apache proxy, if you need a password ask Ilija Vukotic.\n\n\n\n\nIT-ES\n\n\nMonit",
            "title": "Elasticsearch"
        },
        {
            "location": "/infrastructure/elasticsearch/#elasticsearch",
            "text": "",
            "title": "Elasticsearch"
        },
        {
            "location": "/infrastructure/elasticsearch/#uchicago",
            "text": "",
            "title": "UChicago"
        },
        {
            "location": "/infrastructure/elasticsearch/#infrastructure",
            "text": "5 data nodes, 3 service nodes (3 masters, 2 kibana), 15 TB of SSDs. Will be expanded to:...  backup: each Sunday all data is incrementally backed up.",
            "title": "Infrastructure"
        },
        {
            "location": "/infrastructure/elasticsearch/#kibana",
            "text": "Production   Production version  is fully open but only for read access. Searches, visualization, dashboards can be created but can not be saved. To add something to it one has to create it first in the development and ask Ilija to copy it over to the production Kibana.   Development Development version  is read/write capable but sits behind a apache proxy, if you need a password ask Ilija Vukotic.",
            "title": "Kibana"
        },
        {
            "location": "/infrastructure/elasticsearch/#it-es",
            "text": "",
            "title": "IT-ES"
        },
        {
            "location": "/infrastructure/elasticsearch/#monit",
            "text": "",
            "title": "Monit"
        },
        {
            "location": "/infrastructure/hadoop/",
            "text": "",
            "title": "Hadoop"
        },
        {
            "location": "/infrastructure/logstash/",
            "text": "",
            "title": "Logstash"
        },
        {
            "location": "/user-guide/es_and_kibana/",
            "text": "ES and Kibana",
            "title": "Elasticsearch & Kibana"
        },
        {
            "location": "/user-guide/es_and_kibana/#es-and-kibana",
            "text": "",
            "title": "ES and Kibana"
        },
        {
            "location": "/user-guide/hadoop/",
            "text": "",
            "title": "Hadoop"
        },
        {
            "location": "/user-guide/spark/",
            "text": "",
            "title": "Spark"
        },
        {
            "location": "/user-guide/pig/",
            "text": "",
            "title": "Pig Latin"
        },
        {
            "location": "/user-guide/jupyter/",
            "text": "",
            "title": "Jupyter"
        },
        {
            "location": "/about/",
            "text": "ATLAS analytics\n\n\nv 0.1.1\n\n\nIlija Vukotic\n\n\ncurrent date",
            "title": "About"
        },
        {
            "location": "/about/#atlas-analytics",
            "text": "",
            "title": "ATLAS analytics"
        },
        {
            "location": "/about/#v-011",
            "text": "",
            "title": "v 0.1.1"
        },
        {
            "location": "/about/#ilija-vukotic",
            "text": "",
            "title": "Ilija Vukotic"
        },
        {
            "location": "/about/#current-date",
            "text": "",
            "title": "current date"
        }
    ]
}